{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10be998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "file_path = r'D:\\Final_Projects\\Copper_Set.csv.csv'\n",
    "save_path = r'D:\\Final_Projects'\n",
    "output_file = os.path.join(save_path, 'Cleaned_Copper_modelling_project.csv')\n",
    "\n",
    "# Load the dataset with error handling and specify data types to prevent dtype warnings\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at {file_path} was not found.\")\n",
    "    exit()\n",
    "\n",
    "# Step 1: Initial Inspection\n",
    "print(\"Dataset Information:\\n\", df.info())\n",
    "print(\"\\nFirst 5 rows:\\n\", df.head())\n",
    "print(\"\\nSummary Statistics:\\n\", df.describe())\n",
    "\n",
    "# Check unique values in the 'status' column\n",
    "if 'status' in df.columns:\n",
    "    unique_status_values = df['status'].unique()\n",
    "    print(\"\\nUnique values in 'status' column:\", unique_status_values)\n",
    "\n",
    "# Step 2: Handle Non-Numeric Values in Numeric Columns\n",
    "# Convert non-numeric values to NaN for each numeric column\n",
    "numeric_columns = ['item_date', 'quantity tons', 'customer', 'country', 'application', 'thickness', 'width', 'delivery date', 'selling_price']\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Step 3: Handle Missing Values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values in each column:\\n\", missing_values)\n",
    "\n",
    "# Drop columns with a high percentage of missing values\n",
    "threshold = 0.5  # Adjust threshold as needed\n",
    "df = df.dropna(axis=1, thresh=int((1 - threshold) * len(df)))\n",
    "\n",
    "# Impute remaining missing values based on column type\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0:  # Only act on columns with missing values\n",
    "        if df[col].dtype in ['float64', 'int64']:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "        elif df[col].dtype == 'object':\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Step 4: Remove Duplicates\n",
    "initial_row_count = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "print(f\"\\nDuplicates removed: {initial_row_count - df.shape[0]} rows\")\n",
    "\n",
    "# Step 5: Standardize Column Formats\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Convert 'item_date' column to datetime format\n",
    "if 'item_date' in df.columns:\n",
    "    df['item_date'] = pd.to_datetime(df['item_date'], format='%d%m%y', errors='coerce')\n",
    "    print(\"\\nConverted 'item_date' column to datetime format.\")\n",
    "\n",
    "# Step 6: Detect and Handle Negative Values in 'selling_price'\n",
    "if 'selling_price' in df.columns:\n",
    "    negative_prices = df[df['selling_price'] < 0]\n",
    "    if not negative_prices.empty:\n",
    "        print(f\"\\nNegative values detected in 'selling_price': {negative_prices.shape[0]} rows\")\n",
    "    df.loc[df['selling_price'] < 0, 'selling_price'] = None\n",
    "    df['selling_price'].fillna(df['selling_price'].median(), inplace=True)\n",
    "\n",
    "# Step 7: Outlier Detection and Treatment\n",
    "for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify outliers\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    if not outliers.empty:\n",
    "        print(f\"\\nOutliers detected in column '{col}': {outliers.shape[0]} rows\")\n",
    "    \n",
    "    df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "\n",
    "# Step 8: Save the Cleaned Dataset\n",
    "try:\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nCleaned dataset saved to {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddcbbe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = r'D:/Final_Projects/Cleaned_Copper_modelling_project.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the first few rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Inspect the column names\n",
    "print(\"\\nColumn names in the dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Get basic information about the dataset (data types, non-null counts)\n",
    "print(\"\\nDataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Get summary statistics of the numerical columns\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for any missing values in each column\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check unique values in categorical columns (like 'status')\n",
    "print(\"\\nUnique values in 'status' column:\")\n",
    "print(df['status'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'D:/Final_Projects/Cleaned_Copper_modelling_project.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the 'item_date' column as it contains all missing values\n",
    "df = df.drop(columns=['item_date'])\n",
    "\n",
    "# Handle negative values in 'quantity tons' by removing rows with negative quantities\n",
    "df = df[df['quantity tons'] >= 0]\n",
    "\n",
    "# Encode categorical columns 'status' and 'item type' using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['status', 'item type'], drop_first=True)\n",
    "\n",
    "# Check if there are any other missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "new_file_path = r'D:/Final_Projects/New_Cleaned_Copper_modelling_project.csv'\n",
    "df.to_csv(new_file_path, index=False)\n",
    "\n",
    "# Check the cleaned dataset\n",
    "print(\"\\nFirst 5 rows of the cleaned dataset:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56074fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, confusion_matrix\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "# Set up logging to both console and file\n",
    "log_file = 'script_logs.log'\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create a file handler for logging to a file\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the file handler to the root logger\n",
    "logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "# Create a console handler for logging to the console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the console handler to the root logger\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "# Log the start of the script\n",
    "logging.info(\"Starting data preprocessing and model training script.\")\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = r'D:/Final_Projects/New_Cleaned_Copper_modelling_project.csv'\n",
    "logging.info(f\"Loading dataset from {file_path}...\")\n",
    "df = pd.read_csv(file_path)\n",
    "logging.info(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Check for the columns in the dataset\n",
    "logging.info(f\"Columns in dataset: {df.columns}\\n\")\n",
    "\n",
    "# Check for skewness\n",
    "logging.info(f\"Skewness in numerical features:\\n{df.skew()}\\n\")\n",
    "\n",
    "# Visualizing skewness with histograms\n",
    "logging.info(\"Visualizing skewness in numerical features...\")\n",
    "df.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()\n",
    "\n",
    "# Checking for outliers using boxplots\n",
    "logging.info(\"Checking for outliers in numerical features...\")\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=df[feature])\n",
    "    plt.title(f\"Boxplot of {feature}\")\n",
    "    plt.show()\n",
    "\n",
    "# 2. Data Transformation and Cleaning\n",
    "# Check for missing values\n",
    "logging.info(f\"Missing values:\\n{df.isnull().sum()}\\n\")\n",
    "\n",
    "# Fill missing values for numerical columns only\n",
    "df[numerical_features] = df[numerical_features].fillna(df[numerical_features].mean())\n",
    "\n",
    "# Separate features and target variables\n",
    "logging.info(\"Separating features and target variables.\")\n",
    "if 'status_Won' in df.columns:\n",
    "    # For regression: Selling_Price as target\n",
    "    X = df.drop(['selling_price', 'id', 'material_ref', 'product_ref', 'status_Lost', 'status_Not lost for AM',\n",
    "                'status_Offerable', 'status_Offered', 'status_Revised', 'status_To be approved',\n",
    "                'status_Wonderful', 'item type_Others', 'item type_PL', 'item type_S', 'item type_SLAWR',\n",
    "                'item type_W', 'item type_WI'], axis=1)  # Drop non-relevant columns\n",
    "    \n",
    "    y_regression = df['selling_price']\n",
    "\n",
    "    # For classification: status_Won as target (1 for 'Won', 0 for others)\n",
    "    y_classification = df['status_Won'].map({1: 1, 0: 0})  # Ensure the target is binary\n",
    "else:\n",
    "    # Handle the case when 'status_Won' is missing (for regression only)\n",
    "    logging.warning(\"status_Won column not found. Proceeding with regression only.\")\n",
    "    X = df.drop(['selling_price', 'id', 'material_ref', 'product_ref'], axis=1)  # Drop non-relevant columns\n",
    "    y_regression = df['selling_price']\n",
    "    y_classification = None  # Set to None as classification is not possible\n",
    "\n",
    "# One-hot encode categorical variables if necessary\n",
    "logging.info(\"One-hot encoding categorical variables...\")\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Handle skewness for regression target variable\n",
    "# Apply log transformation if 'selling_price' is skewed\n",
    "if df['selling_price'].skew() > 0.5:  # Adjust threshold as needed\n",
    "    logging.info(\"Selling Price is skewed. Applying log transformation.\")\n",
    "    y_regression = np.log1p(y_regression)  # Log transformation\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "logging.info(\"Splitting the dataset into train and test sets.\")\n",
    "X_train, X_test, y_train_regression, y_test_regression = train_test_split(X, y_regression, test_size=0.2, random_state=42)\n",
    "\n",
    "# Only split for classification if 'status_Won' exists\n",
    "if y_classification is not None:\n",
    "    _, _, y_train_classification, y_test_classification = train_test_split(X, y_classification, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    y_train_classification = y_test_classification = None\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "logging.info(\"Applying feature scaling (standardization)...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Machine Learning Models\n",
    "\n",
    "## Regression Model - RandomForestRegressor\n",
    "logging.info(\"Training RandomForestRegressor for regression task.\")\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "rf_regressor.fit(X_train_scaled, y_train_regression)\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "logging.info(\"Making predictions and evaluating the RandomForestRegressor model.\")\n",
    "y_pred_regression = rf_regressor.predict(X_test_scaled)\n",
    "mae = mean_absolute_error(y_test_regression, y_pred_regression)\n",
    "mse = mean_squared_error(y_test_regression, y_pred_regression)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_regression, y_pred_regression)\n",
    "\n",
    "logging.info(\"Regression Model (RandomForestRegressor) Performance:\")\n",
    "logging.info(f\"MAE: {mae}\")\n",
    "logging.info(f\"MSE: {mse}\")\n",
    "logging.info(f\"RMSE: {rmse}\")\n",
    "logging.info(f\"R²: {r2}\")\n",
    "\n",
    "# Hyperparameter tuning for the regression model\n",
    "logging.info(\"Performing RandomizedSearchCV for regression model.\")\n",
    "param_dist_regression = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_regression = RandomizedSearchCV(estimator=rf_regressor, param_distributions=param_dist_regression, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "random_search_regression.fit(X_train_scaled, y_train_regression)\n",
    "\n",
    "logging.info(f\"Best Parameters for Regression: {random_search_regression.best_params_}\")\n",
    "best_rf_regressor = random_search_regression.best_estimator_\n",
    "\n",
    "# 4. Classification Model - RandomForestClassifier (only if 'status_Won' exists)\n",
    "if y_classification is not None:\n",
    "    logging.info(\"Training RandomForestClassifier for classification task.\")\n",
    "    # Initialize and train the RandomForestClassifier\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "    rf_classifier.fit(X_train_scaled, y_train_classification)\n",
    "\n",
    "    # Predict and evaluate on the test set\n",
    "    logging.info(\"Making predictions and evaluating the RandomForestClassifier model.\")\n",
    "    y_pred_classification = rf_classifier.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test_classification, y_pred_classification)\n",
    "    conf_matrix = confusion_matrix(y_test_classification, y_pred_classification)\n",
    "\n",
    "    logging.info(\"Classification Model (RandomForestClassifier) Performance:\")\n",
    "    logging.info(f\"Accuracy: {accuracy}\")\n",
    "    logging.info(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "    # Hyperparameter tuning for the classification model\n",
    "    logging.info(\"Performing RandomizedSearchCV for classification model.\")\n",
    "    param_dist_classification = {\n",
    "        'n_estimators': [100, 200, 300, 400],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    random_search_classification = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_dist_classification, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "    random_search_classification.fit(X_train_scaled, y_train_classification)\n",
    "\n",
    "    logging.info(f\"Best Parameters for Classification: {random_search_classification.best_params_}\")\n",
    "    best_rf_classifier = random_search_classification.best_estimator_\n",
    "\n",
    "# 5. Saving the Best Models and Scaler\n",
    "model_save_path_regression = 'D:/Final_Projects/random_forest_best_regressor.joblib'\n",
    "model_save_path_classification = 'D:/Final_Projects/random_forest_best_classifier.joblib'\n",
    "scaler_save_path = 'D:/Final_Projects/scaler.joblib'\n",
    "\n",
    "logging.info(f\"Saving the best regression model to {model_save_path_regression}.\")\n",
    "joblib.dump(best_rf_regressor, model_save_path_regression)\n",
    "\n",
    "if y_classification is not None:\n",
    "    logging.info(f\"Saving the best classification model to {model_save_path_classification}.\")\n",
    "    joblib.dump(best_rf_classifier, model_save_path_classification)\n",
    "\n",
    "logging.info(f\"Saving the scaler to {scaler_save_path}.\")\n",
    "joblib.dump(scaler, scaler_save_path)\n",
    "\n",
    "logging.info(\"Model training and saving completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9666482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, confusion_matrix\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "# Set up logging to both console and file\n",
    "log_file = 'script_logs.log'\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create a file handler for logging to a file\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the file handler to the root logger\n",
    "logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "# Create a console handler for logging to the console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the console handler to the root logger\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "# Log the start of the script\n",
    "logging.info(\"Starting data preprocessing and model training script.\")\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = r'D:/Final_Projects/New_Cleaned_Copper_modelling_project.csv'\n",
    "logging.info(f\"Loading dataset from {file_path}...\")\n",
    "df = pd.read_csv(file_path)\n",
    "logging.info(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Check for the columns in the dataset\n",
    "logging.info(f\"Columns in dataset: {df.columns}\\n\")\n",
    "\n",
    "# Format 'delivery_date' to DD/MM/YY if exists\n",
    "if 'delivery_date' in df.columns:\n",
    "    df['delivery_date'] = pd.to_datetime(df['delivery_date'], errors='coerce').dt.strftime('%d/%m/%y')\n",
    "    logging.info(\"Delivery date formatted to DD/MM/YY.\")\n",
    "\n",
    "# Check for skewness\n",
    "logging.info(f\"Skewness in numerical features:\\n{df.skew()}\\n\")\n",
    "\n",
    "# Visualizing skewness with histograms\n",
    "logging.info(\"Visualizing skewness in numerical features...\")\n",
    "df.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()\n",
    "\n",
    "# Checking for outliers using boxplots\n",
    "logging.info(\"Checking for outliers in numerical features...\")\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x=df[feature])\n",
    "    plt.title(f\"Boxplot of {feature}\")\n",
    "    plt.show()\n",
    "\n",
    "# 2. Data Transformation and Cleaning\n",
    "# Remove the 'id' column as requested\n",
    "logging.info(\"Removing 'id' column from dataset.\")\n",
    "df = df.drop(columns=['id'], axis=1)\n",
    "\n",
    "# Check for missing values\n",
    "logging.info(f\"Missing values:\\n{df.isnull().sum()}\\n\")\n",
    "\n",
    "# Fill missing values for numerical columns only\n",
    "df[numerical_features] = df[numerical_features].fillna(df[numerical_features].mean())\n",
    "\n",
    "# Separate features and target variables\n",
    "logging.info(\"Separating features and target variables.\")\n",
    "if 'status_Won' in df.columns:\n",
    "    # For regression: Selling_Price as target\n",
    "    X = df.drop(['selling_price', 'material_ref', 'product_ref', 'status_Lost', 'status_Not lost for AM',\n",
    "                'status_Offerable', 'status_Offered', 'status_Revised', 'status_To be approved',\n",
    "                'status_Wonderful', 'item type_Others', 'item type_PL', 'item type_S', 'item type_SLAWR',\n",
    "                'item type_W', 'item type_WI'], axis=1)  # Drop non-relevant columns\n",
    "    \n",
    "    y_regression = df['selling_price']\n",
    "\n",
    "    # For classification: status_Won as target (1 for 'Won', 0 for others)\n",
    "    y_classification = df['status_Won'].map({1: 1, 0: 0})  # Ensure the target is binary\n",
    "else:\n",
    "    # Handle the case when 'status_Won' is missing (for regression only)\n",
    "    logging.warning(\"status_Won column not found. Proceeding with regression only.\")\n",
    "    X = df.drop(['selling_price', 'material_ref', 'product_ref'], axis=1)  # Drop non-relevant columns\n",
    "    y_regression = df['selling_price']\n",
    "    y_classification = None  # Set to None as classification is not possible\n",
    "\n",
    "# One-hot encode categorical variables if necessary\n",
    "logging.info(\"One-hot encoding categorical variables...\")\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Handle skewness for regression target variable\n",
    "# Apply log transformation if 'selling_price' is skewed\n",
    "if df['selling_price'].skew() > 0.5:  # Adjust threshold as needed\n",
    "    logging.info(\"Selling Price is skewed. Applying log transformation.\")\n",
    "    y_regression = np.log1p(y_regression)  # Log transformation\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "logging.info(\"Splitting the dataset into train and test sets.\")\n",
    "X_train, X_test, y_train_regression, y_test_regression = train_test_split(X, y_regression, test_size=0.2, random_state=42)\n",
    "\n",
    "# Only split for classification if 'status_Won' exists\n",
    "if y_classification is not None:\n",
    "    _, _, y_train_classification, y_test_classification = train_test_split(X, y_classification, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    y_train_classification = y_test_classification = None\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "logging.info(\"Applying feature scaling (standardization)...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Machine Learning Models\n",
    "\n",
    "## Regression Model - RandomForestRegressor\n",
    "logging.info(\"Training RandomForestRegressor for regression task.\")\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "rf_regressor.fit(X_train_scaled, y_train_regression)\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "logging.info(\"Making predictions and evaluating the RandomForestRegressor model.\")\n",
    "y_pred_regression = rf_regressor.predict(X_test_scaled)\n",
    "mae = mean_absolute_error(y_test_regression, y_pred_regression)\n",
    "mse = mean_squared_error(y_test_regression, y_pred_regression)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_regression, y_pred_regression)\n",
    "\n",
    "logging.info(\"Regression Model (RandomForestRegressor) Performance:\")\n",
    "logging.info(f\"MAE: {mae}\")\n",
    "logging.info(f\"MSE: {mse}\")\n",
    "logging.info(f\"RMSE: {rmse}\")\n",
    "logging.info(f\"R²: {r2}\")\n",
    "\n",
    "# DecisionTreeRegressor\n",
    "logging.info(\"Training DecisionTreeRegressor for regression task.\")\n",
    "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
    "dt_regressor.fit(X_train_scaled, y_train_regression)\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "y_pred_dt_regression = dt_regressor.predict(X_test_scaled)\n",
    "mae_dt = mean_absolute_error(y_test_regression, y_pred_dt_regression)\n",
    "mse_dt = mean_squared_error(y_test_regression, y_pred_dt_regression)\n",
    "rmse_dt = np.sqrt(mse_dt)\n",
    "r2_dt = r2_score(y_test_regression, y_pred_dt_regression)\n",
    "\n",
    "logging.info(\"Regression Model (DecisionTreeRegressor) Performance:\")\n",
    "logging.info(f\"MAE: {mae_dt}\")\n",
    "logging.info(f\"MSE: {mse_dt}\")\n",
    "logging.info(f\"RMSE: {rmse_dt}\")\n",
    "logging.info(f\"R²: {r2_dt}\")\n",
    "\n",
    "# Hyperparameter tuning for the regression model\n",
    "logging.info(\"Performing RandomizedSearchCV for regression model.\")\n",
    "param_dist_regression = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_regression = RandomizedSearchCV(estimator=rf_regressor, param_distributions=param_dist_regression, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "random_search_regression.fit(X_train_scaled, y_train_regression)\n",
    "\n",
    "logging.info(f\"Best Parameters for Regression: {random_search_regression.best_params_}\")\n",
    "best_rf_regressor = random_search_regression.best_estimator_\n",
    "\n",
    "# 4. Classification Model - RandomForestClassifier (only if 'status_Won' exists)\n",
    "if y_classification is not None:\n",
    "    logging.info(\"Training RandomForestClassifier for classification task.\")\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "    rf_classifier.fit(X_train_scaled, y_train_classification)\n",
    "\n",
    "    # Predict and evaluate on the test set\n",
    "    logging.info(\"Making predictions and evaluating the RandomForestClassifier model.\")\n",
    "    y_pred_classification = rf_classifier.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test_classification, y_pred_classification)\n",
    "    cm = confusion_matrix(y_test_classification, y_pred_classification)\n",
    "\n",
    "    logging.info(\"Classification Model (RandomForestClassifier) Performance:\")\n",
    "    logging.info(f\"Accuracy: {accuracy}\")\n",
    "    logging.info(f\"Confusion Matrix: \\n{cm}\")\n",
    "\n",
    "    # Hyperparameter tuning for the classification model\n",
    "    logging.info(\"Performing RandomizedSearchCV for classification model.\")\n",
    "    param_dist_classification = {\n",
    "        'n_estimators': [100, 200, 300, 400],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    random_search_classification = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_dist_classification, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "    random_search_classification.fit(X_train_scaled, y_train_classification)\n",
    "\n",
    "    logging.info(f\"Best Parameters for Classification: {random_search_classification.best_params_}\")\n",
    "    best_rf_classifier = random_search_classification.best_estimator_\n",
    "\n",
    "# Save models and scaler\n",
    "logging.info(\"Saving models and scaler.\")\n",
    "joblib.dump(best_rf_regressor, 'best_rf_regressor.pkl')\n",
    "joblib.dump(best_rf_classifier, 'best_rf_classifier.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "logging.info(\"Script execution completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7faab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b173568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
